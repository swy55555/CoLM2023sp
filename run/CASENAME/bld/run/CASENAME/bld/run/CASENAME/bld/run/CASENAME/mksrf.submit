#!/bin/bash

#SBATCH -J colm_CASENAME-mksrf
#SBATCH -p cpu_parallel
#SBATCH -o mksrf-%j.out
#SBATCH -e mksrf-%j.err
#SBATCH -N 20
#SBATCH -n 1200
#SBATCH --ntasks-per-node=60
#SBATCH --mem=220G
#SBATCH --exclusive
#SBATCH -t 24:00:00

module purge
module load compiler/intel/2021.3.1
module load mpi/intelmpi/2018.4.274
module load mathlib/netcdf/intel/4.4.1
module load mathlib/hdf5/intel/1.8.20
source /public/software/compiler/intel-compiler/2021.3.0/mkl/env/vars.sh

export I_MPI_FABRICS=shm:dapl
export I_MPI_DAPL_UD=1
export I_MPI_DAPL_UD_RDMA_MIXED=1
export I_MPI_LARGE_SCALE_THRESHOLD=8192
export I_MPI_DAPL_UD_ACK_SEND_POOL_SIZE=8704
export I_MPI_DAPL_UD_ACK_RECV_POOL_SIZE=8704
export I_MPI_DAPL_UD_RNDV_EP_NUM=2

export DAPL_UCM_REP_TIME=8000 #  REQUEST timer, waiting for REPLY in millisecs
export DAPL_UCM_RTU_TIME=8000 #  REPLY timer, waiting for RTU in millisecs
export DAPL_UCM_RETRY=10 #  REQUEST and REPLY retries
export DAPL_UCM_CQ_SIZE=2000
export DAPL_UCM_QP_SIZE=2000

export DAPL_UCM_DREQ_RETRY=4 #default == 1
export DAPL_UCM_DREP_TIME=200 #default == 200ms
export DAPL_UCM_WAIT_TIME=10000 #default == 60000ms

ulimit -s unlimited
scontrol show hostname > bld/run/nd
NP=$SLURM_NPROCS

cd /stu01/sunwy22/CoLM/CoLM202X1/run/CASENAME/bld/run/
mpirun -np $NP -machinefile nd ./mksrfdata.x ../../input_CASENAME.nml > ../../logmksrfdata
